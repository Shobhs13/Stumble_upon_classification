# -*- coding: utf-8 -*-
"""stumbleupon

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/102wJ3L5v8xGaFm4WzSWtbDtn4Qw74ISx
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np 

import matplotlib.pyplot as plt 
import seaborn as sns 
# %matplotlib inline 

import tensorflow as tf

df_train=pd.read_csv('train.tsv',sep='\t')
df_train

df_test = pd.read_csv("test.tsv",sep='\t')
df_test

df_train.head()

df_test.head()

df_train.columns

df_test.columns

df_train.info()

df_test.info()

df_train.describe()

df_test.describe()

df_train.isna().any()

df_test.isna().any()

df_train["alchemy_category"]

alchemy_cat = df_train.groupby('alchemy_category')

alchemy_cat.head()

plt.figure(figsize=(18,10))
sns.countplot(x=df_train['alchemy_category'], hue= df_train['label'])
plt.xlabel('Category')
plt.xticks(rotation=45)

df_train['boilerplate'].replace(to_replace=r'"title":', value = "", inplace = True, regex = True)
df_train['boilerplate'].replace(to_replace=r'"url":',value="",inplace=True,regex=True)
df_train['boilerplate'].replace(to_replace=r'{|}',value="",inplace=True,regex=True)
df_train['boilerplate']=df_train['boilerplate'].str.lower()

df_test['boilerplate'].replace(to_replace=r'"title":', value="",inplace=True,regex=True)
df_test['boilerplate'].replace(to_replace=r'"url":',value="",inplace=True,regex=True)
df_test['boilerplate'].replace(to_replace=r'{|}',value="",inplace=True,regex=True)
df_test['boilerplate']=df_test['boilerplate'].str.lower()

df_train['boilerplate'].head()

df_test['boilerplate']

pip install transformers

from transformers import AutoTokenizer, TFAutoModel


#Downloading the tokenizer and the Albert model for fine tuning

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
bert=TFAutoModel.from_pretrained('bert-base-uncased')

#Defining the sequence length so that it can process 512 at max in time
SEQ_length=512

# X and Y matrix from the Df train set 

Xids=np.zeros((df_train.shape[0],SEQ_length))
Xmask=np.zeros((df_train.shape[0],SEQ_length))
y=np.zeros((df_train.shape[0],1))

#test dataframe

Xids_test=np.zeros((df_test.shape[0],SEQ_length))
Xmask_test=np.zeros((df_test.shape[0],SEQ_length))
Xids

for i,sequence in enumerate(df_train['boilerplate']):
    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,
                           truncation=True,return_token_type_ids=False,return_attention_mask=True,
                           return_tensors='tf')

Xids[i,:],Xmask[i,:],y[i,0]=tokens['input_ids'],tokens['attention_mask'],df_train.loc[i,'label']

for i,sequence in enumerate(df_test['boilerplate']):
    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,
                           truncation=True,return_token_type_ids=False,return_attention_mask=True,
                           return_tensors='tf')

Xids_test[i,:],Xmask_test[i,:]=tokens['input_ids'],tokens['attention_mask']

# GPU is avalaible
tf.config.get_visible_devices()

#Creating dataset
dataset=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))

def func(input_ids,mask,labels):
    return {'input_ids':input_ids,'attention_mask':mask},labels

dataset=dataset.map(func)
dataset=dataset.shuffle(100000).batch(64).prefetch(1000)

DS_size=len(list(dataset))


train=dataset.take(round(DS_size*0.90))
val=dataset.skip(round(DS_size*0.90))

dataset_test=tf.data.Dataset.from_tensor_slices((Xids_test,Xmask_test))

def func(input_ids,mask):
    return {'input_ids':input_ids,'attention_mask':mask}

dataset_test=dataset_test.map(func)
dataset_test=dataset_test.batch(64).prefetch(1000)

from transformers import TFDistilBertModel, DistilBertConfig
distil_bert = 'distilbert-base-uncased'

config = DistilBertConfig(dropout=0.4, attention_dropout=0.4)
config.output_hidden_states = False
transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)

input_ids_in = tf.keras.layers.Input(shape=(SEQ_length,), name='input_ids', dtype='int32')
input_masks_in = tf.keras.layers.Input(shape=(SEQ_length,), name='attention_mask', dtype='int32') 

embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]
X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True, dropout=0.4, recurrent_dropout=0.4))(embedding_layer)
X = tf.keras.layers.GlobalMaxPool1D()(X)
X = tf.keras.layers.Dense(64, activation='relu')(X)
X = tf.keras.layers.Dropout(0.3)(X)
X = tf.keras.layers.Dense(32, activation='relu')(X)
X = tf.keras.layers.Dropout(0.3)(X)
X = tf.keras.layers.Dense(1, activation='sigmoid')(X)
model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)

for layer in model.layers[:3]:
  layer.trainable = False

model.summary()

model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer='adam',metrics=[tf.keras.metrics.AUC(),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()
])

model=model.fit(train,validation_data=val,epochs=10)

predictions=model.predict(dataset_test)
df_test['label']=predictions

df_test.to_csv('submission.csv',columns=['urlid','label'],index=False)

input_x=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))

def map_func(input_ids,mask,labels):
    return {'input_ids':input_ids,'attention_mask':mask}

input_x=input_x.map(map_func)
input_x=input_x.shuffle(100000).batch(32).prefetch(1000)

y_true = y
y_true

y_pred=model.predict(dataset)
y_pred


y_pred = np.round(y_pred)
y_pred


from sklearn import metrics
print(metrics.classification_report(y_true, y_pred))